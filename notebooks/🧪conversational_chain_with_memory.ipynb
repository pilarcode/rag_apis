{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation Chain with Memory\n",
    "\n",
    "LLMs are stateless, which means they process each incoming query in isolation, without considering previous interactions.\n",
    "To overcome this limitation, LangChain offers a standard interface for memory, a variety of memory implementations and examples\n",
    "of chains and agents that employ memory:\n",
    "* ConversationBufferMemory: Stores the entire conversation history as a single string\n",
    "* ConversationBufferWindowMemory: Maintains a memory window that keeps a limited number of past interactions.\n",
    "* ConversationSummaryMemory: Holds a summary of previous converations.\n",
    "* ConversationSummaryBufferMemory: This method combines the features: of ConversationSummaryMemory and ConversationBufferWindowMemory.Stores the conversation's summary and buffer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/mrag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory,ConversationBufferWindowMemory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate,PromptTemplate\n",
    "from langchain import ConversationChain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "from llm_instances import get_embeddings_instance, get_llm_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LLM instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = get_llm_instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_function = get_embeddings_instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ConversationChain \n",
    "\n",
    "By default, LangChain's ConversationChain has a simple type of memory that remembers all previous inputs/outputs and adds them to the context that is passed.\n",
    "It can also remember what the initial message were after 3 questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation_= ConversationChain(\n",
    "    llm=llm_model,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human:  what's The capital of France?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris. It is the largest city in the country and is known for its iconic landmarks such as the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum. Paris is also a major center for art, fashion, and cuisine.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_.predict(input=\" what's The capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human:  what's The capital of France?\n",
      "AI: The capital of France is Paris. It is the largest city in the country and is known for its iconic landmarks such as the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum. Paris is also a major center for art, fashion, and cuisine.\n",
      "Human:  and Italy?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The capital of Italy is Rome. It is a city with a rich history, known for its ancient ruins such as the Colosseum and the Roman Forum, as well as its art and architecture, including the Vatican City and St. Peter's Basilica. Rome is also famous for its delicious cuisine, including pasta, pizza, and gelato.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "conversation_.predict(input=\" and Italy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human:  what's The capital of France?\n",
      "AI: The capital of France is Paris. It is the largest city in the country and is known for its iconic landmarks such as the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum. Paris is also a major center for art, fashion, and cuisine.\n",
      "Human:  and Italy?\n",
      "AI: The capital of Italy is Rome. It is a city with a rich history, known for its ancient ruins such as the Colosseum and the Roman Forum, as well as its art and architecture, including the Vatican City and St. Peter's Basilica. Rome is also famous for its delicious cuisine, including pasta, pizza, and gelato.\n",
      "Human:  and Spain?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of Spain is Madrid. It is the largest city in the country and is known for its rich cultural heritage, including the Royal Palace, the Prado Museum, and the Retiro Park. Madrid is also famous for its vibrant nightlife and delicious tapas.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "conversation_.predict(input=\" and Spain?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm_model, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: The following is a friendly conversation between a human and an AI.\n",
      "Human: Tell me how to write an endpoint in python.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Tell me how to write an endpoint in python.', 'history': [HumanMessage(content='Tell me how to write an endpoint in python.'), AIMessage(content='To write an endpoint in Python, you can use a web framework such as Flask or Django. Here\\'s a simple example using Flask:\\n\\n```python\\nfrom flask import Flask\\n\\napp = Flask(__name__)\\n\\n@app.route(\\'/hello\\', methods=[\\'GET\\'])\\ndef hello():\\n    return \\'Hello, World!\\'\\n\\nif __name__ == \\'__main__\\':\\n    app.run()\\n```\\n\\nIn this example, we define a route `/hello` that responds to GET requests with the message \"Hello, World!\". When you run the Flask app, it will start a web server and listen for incoming requests to this endpoint.\\n\\nYou can then test this endpoint by making a GET request to `http://localhost:5000/hello` (assuming you\\'re running the app locally on port 5000).')], 'response': 'To write an endpoint in Python, you can use a web framework such as Flask or Django. Here\\'s a simple example using Flask:\\n\\n```python\\nfrom flask import Flask\\n\\napp = Flask(__name__)\\n\\n@app.route(\\'/hello\\', methods=[\\'GET\\'])\\ndef hello():\\n    return \\'Hello, World!\\'\\n\\nif __name__ == \\'__main__\\':\\n    app.run()\\n```\\n\\nIn this example, we define a route `/hello` that responds to GET requests with the message \"Hello, World!\". When you run the Flask app, it will start a web server and listen for incoming requests to this endpoint.\\n\\nYou can then test this endpoint by making a GET request to `http://localhost:5000/hello` (assuming you\\'re running the app locally on port 5000).'}\n"
     ]
    }
   ],
   "source": [
    "user_message = \"Tell me how to write an endpoint in python.\"\n",
    "response = conversation(user_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input', 'history', 'response'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me how to write an endpoint in python.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Tell me how to write an endpoint in python.'),\n",
       " AIMessage(content='To write an endpoint in Python, you can use a web framework such as Flask or Django. Here\\'s a simple example using Flask:\\n\\n```python\\nfrom flask import Flask\\n\\napp = Flask(__name__)\\n\\n@app.route(\\'/hello\\', methods=[\\'GET\\'])\\ndef hello():\\n    return \\'Hello, World!\\'\\n\\nif __name__ == \\'__main__\\':\\n    app.run()\\n```\\n\\nIn this example, we define a route `/hello` that responds to GET requests with the message \"Hello, World!\". When you run the Flask app, it will start a web server and listen for incoming requests to this endpoint.\\n\\nYou can then test this endpoint by making a GET request to `http://localhost:5000/hello` (assuming you\\'re running the app locally on port 5000).')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To write an endpoint in Python, you can use a web framework such as Flask or Django. Here\\'s a simple example using Flask:\\n\\n```python\\nfrom flask import Flask\\n\\napp = Flask(__name__)\\n\\n@app.route(\\'/hello\\', methods=[\\'GET\\'])\\ndef hello():\\n    return \\'Hello, World!\\'\\n\\nif __name__ == \\'__main__\\':\\n    app.run()\\n```\\n\\nIn this example, we define a route `/hello` that responds to GET requests with the message \"Hello, World!\". When you run the Flask app, it will start a web server and listen for incoming requests to this endpoint.\\n\\nYou can then test this endpoint by making a GET request to `http://localhost:5000/hello` (assuming you\\'re running the app locally on port 5000).'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nota: Analizamos la respuesta obtenida con  ConversationChain.Vemos que se guarda el input, el historial y la respuesta del LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Act as friendly, professional and polite expert in API specifications. \n",
    "You will provided with a question and a context. Your task is to answer the question based on the context and in the language provided.\n",
    "If the question can't be answered based on the context,  say you don't know.\n",
    "\n",
    "{chat_history}\n",
    "Context: {context}\n",
    "Language: {language}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "question = \"¿Hay algun servicio para crear una mascota?\"\n",
    "chat_history=\"\"\n",
    "language = \"English\"\n",
    "\n",
    "# create a prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"context\",\"language\",\"question\"], \n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_with_memory = ConversationChain(\n",
    "    llm=llm_model,\n",
    "    memory = ConversationBufferWindowMemory(k=3, return_messages=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '¿Hay algun servicio para crear una mascota?',\n",
       " 'history': [],\n",
       " 'response': 'Sí, hay varios servicios en línea que te permiten crear tu propia mascota virtual. Puedes personalizar su apariencia, personalidad y habilidades. Algunos de estos servicios incluso te permiten interactuar con tu mascota a través de juegos y actividades virtuales.'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_memory(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
